{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from env_hiv import HIVPatient\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TimeLimit(\n",
    "    env=HIVPatient(domain_randomization=False), max_episode_steps=200\n",
    ")  # The time wrapper limits the number of steps in an episode at 200.\n",
    "# Now is the floor is yours to implement the agent and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.capacity = capacity # capacity of the buffer\n",
    "        self.data = []\n",
    "        self.index = 0 # index of the next cell to be filled\n",
    "        self.device = device\n",
    "    def append(self, s, a, r, s_, d):\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(None)\n",
    "        self.data[self.index] = (s, a, r, s_, d)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.data, batch_size)\n",
    "        return list(map(lambda x:torch.Tensor(np.array(x)).to(self.device), list(zip(*batch))))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "def greedy_action(network, state):\n",
    "    device = \"cuda\" if next(network.parameters()).is_cuda else \"cpu\"\n",
    "    with torch.no_grad():\n",
    "        Q = network(torch.Tensor(state).unsqueeze(0).to(device))\n",
    "        return torch.argmax(Q).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have to implement your own agent.\n",
    "# Don't modify the methods names and signatures, but you can add methods.\n",
    "# ENJOY!\n",
    "class ProjectAgent:\n",
    "\n",
    "    def __init__(self, config, model, device):\n",
    "\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.nb_actions = config[\"nb_actions\"]\n",
    "        self.memory = ReplayBuffer(config[\"buffer_size\"], device)\n",
    "        self.epsilon_max = config[\"epsilon_max\"]\n",
    "        self.epsilon_min = config[\"epsilon_min\"]\n",
    "        self.epsilon_stop = config[\"epsilon_decay_period\"]\n",
    "        self.epsilon_delay = config[\"epsilon_delay_decay\"]\n",
    "        self.epsilon_step = (self.epsilon_max - self.epsilon_min) / self.epsilon_stop\n",
    "        self.model = model\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=config[\"learning_rate\"]\n",
    "        )\n",
    "\n",
    "        pass\n",
    "\n",
    "    def act(self, observation, use_random=False):\n",
    "\n",
    "        action = greedy_action(self.model, observation)\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def save(self, path):\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "    def gradient_step(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            X, A, R, Y, D = self.memory.sample(self.batch_size)\n",
    "            QYmax = self.model(Y).max(1)[0].detach()\n",
    "            # update = torch.addcmul(R, self.gamma, 1-D, QYmax)\n",
    "            update = torch.addcmul(R, 1 - D, QYmax, value=self.gamma)\n",
    "            QXA = self.model(X).gather(1, A.to(torch.long).unsqueeze(1))\n",
    "            loss = self.criterion(QXA, update.unsqueeze(1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def train(self, env, max_episode):\n",
    "        episode_return = []\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "\n",
    "        with tqdm(total=100) as pbar:\n",
    "            while episode < max_episode:\n",
    "                pbar.update(10)\n",
    "                # update epsilon\n",
    "                if step > self.epsilon_delay:\n",
    "                    epsilon = max(self.epsilon_min, epsilon - self.epsilon_step)\n",
    "\n",
    "                # select epsilon-greedy action\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = greedy_action(self.model, state)\n",
    "\n",
    "                # step\n",
    "                next_state, reward, done, trunc, _ = env.step(action)\n",
    "                self.memory.append(state, action, reward, next_state, done)\n",
    "                episode_cum_reward += reward\n",
    "\n",
    "                # train\n",
    "                self.gradient_step()\n",
    "\n",
    "                # next transition\n",
    "                step += 1\n",
    "                if done or trunc:\n",
    "                    episode += 1\n",
    "                    print(\n",
    "                        \"Episode \",\n",
    "                        \"{:3d}\".format(episode),\n",
    "                        \", epsilon \",\n",
    "                        \"{:6.2f}\".format(epsilon),\n",
    "                        \", batch size \",\n",
    "                        \"{:5d}\".format(len(self.memory)),\n",
    "                        \", episode return \",\n",
    "                        \"{:4.1f}\".format(episode_cum_reward),\n",
    "                        sep=\"\",\n",
    "                    )\n",
    "                    state, _ = env.reset()\n",
    "                    episode_return.append(episode_cum_reward)\n",
    "                    episode_cum_reward = 0\n",
    "                else:\n",
    "                    state = next_state\n",
    "\n",
    "        return episode_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare network\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "nb_neurons = 24\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# DQN config\n",
    "config = {\n",
    "    \"nb_actions\": n_action,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"gamma\": 0.95,\n",
    "    \"buffer_size\": 1000000,\n",
    "    \"epsilon_min\": 0.01,\n",
    "    \"epsilon_max\": 1.0,\n",
    "    \"epsilon_decay_period\": 1000,\n",
    "    \"epsilon_delay_decay\": 20,\n",
    "    \"batch_size\": 20,\n",
    "}\n",
    "\n",
    "DQN = torch.nn.Sequential(\n",
    "    nn.Linear(state_dim, nb_neurons),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(nb_neurons, nb_neurons),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(nb_neurons, n_action),\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011b655e516e4cfd9b054c416495e5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   1, epsilon   0.82, batch size   200, episode return 11292609.8\n",
      "Episode   2, epsilon   0.62, batch size   400, episode return 10298372.7\n",
      "Episode   3, epsilon   0.43, batch size   600, episode return 9000151.1\n",
      "Episode   4, epsilon   0.23, batch size   800, episode return 6375402.7\n",
      "Episode   5, epsilon   0.03, batch size  1000, episode return 4140289.0\n",
      "Episode   6, epsilon   0.01, batch size  1200, episode return 3696070.8\n",
      "Episode   7, epsilon   0.01, batch size  1400, episode return 3671146.9\n",
      "Episode   8, epsilon   0.01, batch size  1600, episode return 3652153.3\n",
      "Episode   9, epsilon   0.01, batch size  1800, episode return 3672928.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m agent \u001b[38;5;241m=\u001b[39m ProjectAgent(config, DQN, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m      3\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m----> 4\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(scores)\n",
      "Cell \u001b[1;32mIn[39], line 71\u001b[0m, in \u001b[0;36mProjectAgent.train\u001b[1;34m(self, env, max_episode)\u001b[0m\n\u001b[0;32m     68\u001b[0m     action \u001b[38;5;241m=\u001b[39m greedy_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, state)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# step\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m next_state, reward, done, trunc, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mappend(state, action, reward, next_state, done)\n\u001b[0;32m     73\u001b[0m episode_cum_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\RalfC\\anaconda3\\envs\\mva_geometric\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\RalfC\\Desktop\\cours_MVA\\rl-class-assignment-RalfCortes\\src\\env_hiv.py:231\u001b[0m, in \u001b[0;36mHIVPatient.step\u001b[1;34m(self, a_index)\u001b[0m\n\u001b[0;32m    229\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate()\n\u001b[0;32m    230\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_set[a_index]\n\u001b[1;32m--> 231\u001b[0m state2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m rew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(state, action, state2)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipping:\n",
      "File \u001b[1;32mc:\\Users\\RalfC\\Desktop\\cours_MVA\\rl-class-assignment-RalfCortes\\src\\env_hiv.py:212\u001b[0m, in \u001b[0;36mHIVPatient.transition\u001b[1;34m(self, state, action, duration)\u001b[0m\n\u001b[0;32m    210\u001b[0m nb_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(duration \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_steps):\n\u001b[1;32m--> 212\u001b[0m     der \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m     state1 \u001b[38;5;241m=\u001b[39m state0 \u001b[38;5;241m+\u001b[39m der \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# np.clip(state1, self.lower, self.upper, out=state1)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train agent\n",
    "agent = ProjectAgent(config, DQN, device=device)\n",
    "env.reset()\n",
    "scores = agent.train(env, max_episode=200)\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mva_geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
